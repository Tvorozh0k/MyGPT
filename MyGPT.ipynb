{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qgqxv1Y3Z9fw",
        "02uQkYWZEhGC",
        "e6QxhtvMydZY",
        "WZqCmC57cs7L",
        "4CX3kxh2gUFV",
        "tOxNTrdji8Ya"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwlfn6Dap+l0uq6mkt9TYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tvorozh0k/MyGPT/blob/main/MyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подключение библиотек"
      ],
      "metadata": {
        "id": "KigzMZ_8NJA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vjs3u1zp_h7b"
      },
      "outputs": [],
      "source": [
        "#@title Подключение библиотеки PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Работа с трансформерами из Hugging Face\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv0BA7ez5hxJ",
        "outputId": "5aabfe85-4430-426d-e79d-ba9c1d0cdf84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQR7nRIvssOS",
        "outputId": "144ea11d-8e2b-4c97-cd90-5ce556b607cc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yVDxYjmH_7zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Теория"
      ],
      "metadata": {
        "id": "bxG_4hrBZ7Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Архитектура GPT2"
      ],
      "metadata": {
        "id": "HVPA3vxgaTmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2LMHeadModel\n",
        "\n",
        "checkpoint = 'gpt2'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3LpsrVCaanD",
        "outputId": "8c5a9410-fde5-4aa9-a526-da5b7398bf46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2SdpaAttention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Как работает `Dropout`?\n",
        "\n",
        "На вход подается `tensor`. Слой `nn.Dropout(p)` проходится по всем элементам тензора и с вероятностью `p` обнуляет элементы. Все элементы, также, умножаются на значение $\\frac{1}{1-p}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "qgqxv1Y3Z9fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{bmatrix}0.9225 & 1.3627 \\\\ 0.2685 & 1.1718\\end{bmatrix} \\to {\\text{nn.Dropout(0.2)}} \\to \\begin{bmatrix}0.9225 \\cdot \\frac{1}{1-0.2} & 0 \\\\ 0.2685 \\cdot \\frac{1}{1-0.2} & 1.1718 \\cdot \\frac{1}{1-0.2}\\end{bmatrix} = \\begin{bmatrix}1.1531 & 0 \\\\ 0.3357 & 1.4647\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "HehiSxaFbcWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почему умножение происходит на $\\frac{1}{1-p}$? Пусть дан тензор размерности $n \\times m$, все элементы которого равны $v$. Тогда, сумма элементов тензора равна $nmv$. После применения `dropout` с параметром $p$ математическое ожидание суммы равно $(1-p)nmv$ (сумма оставшихся ненулевых элементов). Таким образом, мы потеряли в сумме, мы потеряли ненулевые значения, и теперь мы хотим их перераспределить так, чтобы сумма осталось примерно такой же. Для этого нужно все оставшиеся ненулевые элементы в тензоре умножить на $${\\frac{1}{1-p}}$$. Тогда сумма: $\\frac{1}{1-p}(1-p)nmv = nmv$."
      ],
      "metadata": {
        "id": "ITiYzu3Bc7Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример\n",
        "\n",
        "d = nn.Dropout(p=0.2)\n",
        "\n",
        "input_tensor = torch.randn(2, 2)\n",
        "\n",
        "print(f\"[BEFORE DROPOUT]:\")\n",
        "print(f\"input_tensor:\\n{input_tensor}\")\n",
        "\n",
        "print(f\"\\n[AFTER DROPOUT]:\")\n",
        "print(f\"input_tensor:\\n{d(input_tensor)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5BqqpdBanrR",
        "outputId": "afa29211-4592-4ef3-91ff-6b98c5b3474d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BEFORE DROPOUT]:\n",
            "input_tensor:\n",
            "tensor([[0.9225, 1.3627],\n",
            "        [0.2685, 1.1718]])\n",
            "\n",
            "[AFTER DROPOUT]:\n",
            "input_tensor:\n",
            "tensor([[1.1531, 0.0000],\n",
            "        [0.3357, 1.4647]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Арифметические операции над тензорами высшего порядка"
      ],
      "metadata": {
        "id": "02uQkYWZEhGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Перемножение трехмерных тензоров"
      ],
      "metadata": {
        "id": "AVD560iRFuCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$A = \\begin{bmatrix}A_1 & A_2 & \\dots & A_n\\end{bmatrix}, \\;\\;B = \\begin{bmatrix}B_1 & B_2 & \\dots & B_n\\end{bmatrix} \\quad \\to \\quad C = AB = \\begin{bmatrix}A_1 \\cdot B_1 & A_2 \\cdot B_2 & \\dots & A_n \\cdot B_n\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "gub5Cu5bGqUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$A: (n, m, p), \\;\\; B: (n, p, s) \\to C = AB: (n, m, s)$$"
      ],
      "metadata": {
        "id": "DcUrHjubHcAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randint(5, (2, 2, 3))\n",
        "print(f\"a[2, 2, 3]:\\n{a}\\n\")\n",
        "\n",
        "b = torch.randint(5, (2, 3, 2))\n",
        "print(f\"b[2, 3, 2]:\\n{b}\\n\")\n",
        "\n",
        "c = a @ b\n",
        "print(f\"c[2, 2, 2]:\\n{c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5xmRPqiElto",
        "outputId": "006ee8bd-726d-47f0-8374-4c68f78a28b5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a[2, 2, 3]:\n",
            "tensor([[[2, 2, 2],\n",
            "         [4, 4, 1]],\n",
            "\n",
            "        [[3, 3, 2],\n",
            "         [3, 3, 1]]])\n",
            "\n",
            "b[2, 3, 2]:\n",
            "tensor([[[1, 1],\n",
            "         [1, 2],\n",
            "         [1, 3]],\n",
            "\n",
            "        [[1, 1],\n",
            "         [2, 3],\n",
            "         [2, 4]]])\n",
            "\n",
            "c[2, 2, 2]:\n",
            "tensor([[[ 6, 12],\n",
            "         [ 9, 15]],\n",
            "\n",
            "        [[13, 20],\n",
            "         [11, 16]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Аналогично и с другими измерениями (4 и выше)"
      ],
      "metadata": {
        "id": "TJO0kkPioVHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Транспонирование тензоров"
      ],
      "metadata": {
        "id": "ATtHaM0GHzaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn((1, 2, 3, 4))\n",
        "print(f\"Before: {a.shape}\")\n",
        "\n",
        "a = a.transpose(1, 2)\n",
        "print(f\"After: {a.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2crVseIW3p",
        "outputId": "597f59a6-0216-4664-caab-7a2c8f34a487"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: torch.Size([1, 2, 3, 4])\n",
            "After: torch.Size([1, 3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
        "\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "print(a.transpose(0, 1).shape)\n",
        "print(a.transpose(0, 1))\n",
        "\n",
        "print(a.transpose(0, 2).shape)\n",
        "print(a.transpose(0, 2))\n",
        "\n",
        "print(a.transpose(1, 2).shape)\n",
        "print(a.transpose(1, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCkzgDS3pG4V",
        "outputId": "188c66da-e362-47e7-9404-875a001165d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 2])\n",
            "tensor([[[ 1,  2],\n",
            "         [ 3,  4]],\n",
            "\n",
            "        [[ 5,  6],\n",
            "         [ 7,  8]],\n",
            "\n",
            "        [[ 9, 10],\n",
            "         [11, 12]]])\n",
            "torch.Size([2, 3, 2])\n",
            "tensor([[[ 1,  2],\n",
            "         [ 5,  6],\n",
            "         [ 9, 10]],\n",
            "\n",
            "        [[ 3,  4],\n",
            "         [ 7,  8],\n",
            "         [11, 12]]])\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[ 1,  5,  9],\n",
            "         [ 3,  7, 11]],\n",
            "\n",
            "        [[ 2,  6, 10],\n",
            "         [ 4,  8, 12]]])\n",
            "torch.Size([3, 2, 2])\n",
            "tensor([[[ 1,  3],\n",
            "         [ 2,  4]],\n",
            "\n",
            "        [[ 5,  7],\n",
            "         [ 6,  8]],\n",
            "\n",
            "        [[ 9, 11],\n",
            "         [10, 12]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Как работает `scaled_dot_product_attention`?"
      ],
      "metadata": {
        "id": "e6QxhtvMydZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Генерируем случайные матрицы $Q$, $K$ и $V$ и маску $M$\n",
        "\n",
        "q = torch.randn((3, 2))\n",
        "k = torch.randn((3, 2))\n",
        "v = torch.randn((3, 2))\n",
        "\n",
        "m = torch.tril(torch.ones(3, 3))\n",
        "m = m.masked_fill(m==0, float('-inf')) - 1\n",
        "\n",
        "print(f\"q:\\n{q}\\n\")\n",
        "print(f\"k:\\n{k}\\n\")\n",
        "print(f\"v:\\n{v}\\n\")\n",
        "print(f\"m:\\n{m}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anTiGa89ylh_",
        "outputId": "ab607005-28e9-49ed-c15d-b1f282b03142"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q:\n",
            "tensor([[ 0.6891, -0.2827],\n",
            "        [ 1.8924, -0.3903],\n",
            "        [ 1.7954,  1.0172]])\n",
            "\n",
            "k:\n",
            "tensor([[ 1.0728,  1.2521],\n",
            "        [ 0.1752, -0.2029],\n",
            "        [-0.2853, -1.4800]])\n",
            "\n",
            "v:\n",
            "tensor([[ 0.4326, -1.5850],\n",
            "        [ 0.2770, -0.8679],\n",
            "        [ 1.2798, -1.3554]])\n",
            "\n",
            "m:\n",
            "tensor([[0., -inf, -inf],\n",
            "        [0., 0., -inf],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Собственная реализация\n",
        "\n",
        "out = q\n",
        "\n",
        "out @= k.T\n",
        "print(f\"QK^T:\\n{out}\")\n",
        "\n",
        "out *= 2 ** -0.5\n",
        "print(f\"QK^T / sqrt(d_k):\\n{out}\")\n",
        "\n",
        "out += m\n",
        "print(f\"QK^T / sqrt(d_k) + M:\\n{out}\")\n",
        "\n",
        "out = F.softmax(out, dim=-1)\n",
        "print(f\"Softmax(QK^T / sqrt(d_k) + M):\\n{out}\")\n",
        "\n",
        "out @= v\n",
        "print(f\"Softmax(QK^T / sqrt(d_k) + M) * V:\\n{out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2YUmEgczvQh",
        "outputId": "e03fcf8a-b6e8-4905-e889-147415f08c93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QK^T:\n",
            "tensor([[ 0.3853,  0.1781,  0.2218],\n",
            "        [ 1.5415,  0.4108,  0.0377],\n",
            "        [ 3.1996,  0.1082, -2.0177]])\n",
            "QK^T / sqrt(d_k):\n",
            "tensor([[ 0.2724,  0.1259,  0.1569],\n",
            "        [ 1.0900,  0.2904,  0.0267],\n",
            "        [ 2.2625,  0.0765, -1.4267]])\n",
            "QK^T / sqrt(d_k) + M:\n",
            "tensor([[ 0.2724,    -inf,    -inf],\n",
            "        [ 1.0900,  0.2904,    -inf],\n",
            "        [ 2.2625,  0.0765, -1.4267]])\n",
            "Softmax(QK^T / sqrt(d_k) + M):\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.6899, 0.3101, 0.0000],\n",
            "        [0.8792, 0.0988, 0.0220]])\n",
            "Softmax(QK^T / sqrt(d_k) + M) * V:\n",
            "tensor([[ 0.4326, -1.5850],\n",
            "        [ 0.3844, -1.3626],\n",
            "        [ 0.4359, -1.5091]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Готовый метод\n",
        "print(nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=m))\n",
        "print(nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)) # передавать маску НЕ ОБЯЗАТЕЛЬНО"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-oZ3m6W1yAG",
        "outputId": "a656667a-9a50-4d7f-997b-dc13b2373c2c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4326, -1.5850],\n",
            "        [ 0.3844, -1.3626],\n",
            "        [ 0.4359, -1.5091]])\n",
            "tensor([[ 0.4326, -1.5850],\n",
            "        [ 0.3844, -1.3626],\n",
            "        [ 0.4359, -1.5091]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Как работает `LayerNorm`?"
      ],
      "metadata": {
        "id": "WZqCmC57cs7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Генерация данных\n",
        "\n",
        "a = torch.randn((3, 4))\n",
        "\n",
        "print(f\"a:\\n{a}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ZdVVh9cxJi",
        "outputId": "e777ec64-907e-407f-96cd-623da8bbdacd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:\n",
            "tensor([[ 0.7600, -0.7958,  0.1175,  0.8433],\n",
            "        [-0.5222,  2.5007, -1.0763, -1.5622],\n",
            "        [-0.0441, -1.3374,  0.6868, -0.0910]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Собственная реализация\n",
        "\n",
        "print((a[0] - torch.mean(a[0])) / torch.sqrt(torch.var(a[0], correction=0) + 1e-05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENA6qMXheeHr",
        "outputId": "1a9f500c-60d9-451d-efd3-a6c1ee079830"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.8058, -1.5653, -0.1734,  0.9329])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Готовый метод\n",
        "\n",
        "ln = nn.LayerNorm(4)\n",
        "\n",
        "print(f\"LayerNorm weights:\\n{ln.weight}\\n\")\n",
        "print(f\"LayerNorm bias:\\n{ln.bias}\\n\")\n",
        "\n",
        "print(f\"LayerNorm(x):\\n{ln(a)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY8kzxy_dyYD",
        "outputId": "6fd9b73d-3e1f-45ac-ed23-bf9c4a5c78f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayerNorm weights:\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "\n",
            "LayerNorm bias:\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0.], requires_grad=True)\n",
            "\n",
            "LayerNorm(x):\n",
            "tensor([[ 0.8058, -1.5653, -0.1734,  0.9329],\n",
            "        [-0.2258,  1.6846, -0.5759, -0.8829],\n",
            "        [ 0.2094, -1.5686,  1.2143,  0.1449]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Реализация классов"
      ],
      "metadata": {
        "id": "TW9Qak03NPmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. FFN (Feed Forward Network)"
      ],
      "metadata": {
        "id": "4CX3kxh2gUFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Размерности:**\n",
        "\n",
        "**[INPUT]: `(batch_size, sequence_length, n_embd)`**\n",
        "\n",
        "1. `c_fc`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, 4 * n_embd)`\n",
        "2. `gelu`: `(batch_size, sequence_length, 4 * n_embd)` $\\to$ `(batch_size, sequence_length, 4 * n_embd)`\n",
        "3. `c_proj`: `(batch_size, sequence_length, 4 * n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "4. `dropout`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "\n",
        "**[OUTPUT]: `(batch_size, sequence_length, n_embd)`**"
      ],
      "metadata": {
        "id": "tiyhyQRChxs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Реализация класса\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fT7EePo5NR8g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример работы\n",
        "\n",
        "class ExampleConfig:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_embd = 2\n",
        "\n",
        "\n",
        "config = ExampleConfig()\n",
        "ff_layer = FeedForward(config)\n",
        "\n",
        "input_tensor = torch.randn(config.n_embd)\n",
        "print(f\"Input tensor (x): {input_tensor}\")\n",
        "\n",
        "print(\"\\nFeed Forward [FIRST_STEP]:\\n\")\n",
        "\n",
        "# layer parameters\n",
        "print(f\"c_fc.weight:\\n{ff_layer.c_fc.weight}\")\n",
        "print(f\"c_fc.bias:\\n{ff_layer.c_fc.bias}\")\n",
        "\n",
        "# validation\n",
        "print(f\"[CLASS] x = c_fc(x): {ff_layer.c_fc(input_tensor)}\")\n",
        "print(f\"[CHECK]: xW^T + b = {input_tensor @ ff_layer.c_fc.weight.T + ff_layer.c_fc.bias}\")\n",
        "\n",
        "print(\"\\nFeed Forward [SECOND_STEP]:\\n\")\n",
        "\n",
        "# validation\n",
        "print(f\"[CLASS] x = GELU(c_fc(x)): {ff_layer.gelu(ff_layer.c_fc(input_tensor))}\")\n",
        "\n",
        "m = nn.GELU(approximate='tanh')\n",
        "print(f\"[CHECK]: x = GELU(xW^T + b) = {m(input_tensor @ ff_layer.c_fc.weight.T + ff_layer.c_fc.bias)}\")\n",
        "\n",
        "print(\"\\nFeed Forward [THIRD_STEP]:\\n\")\n",
        "\n",
        "# layer parameters\n",
        "print(f\"c_fc.weight:\\n{ff_layer.c_proj.weight}\")\n",
        "print(f\"c_fc.bias:\\n{ff_layer.c_proj.bias}\")\n",
        "\n",
        "# validation\n",
        "print(f\"[CLASS] x = c_proj(GELU(c_fc(x))): {ff_layer.c_proj(ff_layer.gelu(ff_layer.c_fc(input_tensor)))}\")\n",
        "print(f\"[CHECK]: GELU(xW^T + b)W^T + b = {m(input_tensor @ ff_layer.c_fc.weight.T + ff_layer.c_fc.bias) @ ff_layer.c_proj.weight.T + ff_layer.c_proj.bias}\")\n",
        "\n",
        "print(\"\\nFeed Forward [FOURTH_STEP, DROPOUT]:\\n\")\n",
        "print(f\"[CLASS] x = FeedForward(x): {ff_layer.forward(input_tensor)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhX0R24fTXsV",
        "outputId": "1e111c5a-a1b7-467a-8658-8699c675dcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor (x): tensor([-1.5839,  0.0219])\n",
            "\n",
            "Feed Forward [FIRST_STEP]:\n",
            "\n",
            "c_fc.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.5494,  0.1919],\n",
            "        [-0.5304, -0.4630],\n",
            "        [ 0.6974,  0.6517],\n",
            "        [-0.6406, -0.3088],\n",
            "        [ 0.2704,  0.4117],\n",
            "        [-0.5300, -0.2456],\n",
            "        [ 0.1082,  0.5320],\n",
            "        [ 0.6661,  0.2216]], requires_grad=True)\n",
            "c_fc.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.6870,  0.5093, -0.4400,  0.3836, -0.6684, -0.0895,  0.2645, -0.6611],\n",
            "       requires_grad=True)\n",
            "[CLASS] x = c_fc(x): tensor([ 0.1875,  1.3393, -1.5304,  1.3914, -1.0877,  0.7446,  0.1048, -1.7112],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "[CHECK]: xW^T + b = tensor([ 0.1875,  1.3393, -1.5304,  1.3914, -1.0877,  0.7446,  0.1048, -1.7112],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "Feed Forward [SECOND_STEP]:\n",
            "\n",
            "[CLASS] x = GELU(c_fc(x)): tensor([ 0.1077,  1.2182, -0.0966,  1.2770, -0.1507,  0.5746,  0.0568, -0.0746],\n",
            "       grad_fn=<GeluBackward0>)\n",
            "[CHECK]: x = GELU(xW^T + b) = tensor([ 0.1077,  1.2182, -0.0966,  1.2770, -0.1507,  0.5746,  0.0568, -0.0746],\n",
            "       grad_fn=<GeluBackward0>)\n",
            "\n",
            "Feed Forward [THIRD_STEP]:\n",
            "\n",
            "c_fc.weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.2393,  0.1246, -0.3221,  0.0010, -0.0766, -0.3419,  0.1319,  0.0828],\n",
            "        [ 0.2306, -0.0637, -0.0240, -0.0336, -0.1896,  0.0238, -0.1259, -0.0505]],\n",
            "       requires_grad=True)\n",
            "c_fc.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0133, -0.1706], requires_grad=True)\n",
            "[CLASS] x = c_proj(GELU(c_fc(x))): tensor([ 0.0396, -0.2251], grad_fn=<ViewBackward0>)\n",
            "[CHECK]: GELU(xW^T + b)W^T + b = tensor([ 0.0396, -0.2251], grad_fn=<AddBackward0>)\n",
            "\n",
            "Feed Forward [FOURTH_STEP, DROPOUT]:\n",
            "\n",
            "[CLASS] x = FeedForward(x): tensor([ 0.0440, -0.2501], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. MultiHeadAttention"
      ],
      "metadata": {
        "id": "tOxNTrdji8Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Размерности:**\n",
        "\n",
        "**[INPUT]: `(batch_size, sequence_length, n_embd)`**\n",
        "\n",
        "1. `c_attn`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, 3 * n_embd)`\n",
        "2. **[SPLIT]**: `(batch_size, sequence_length, 3 * n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "3. **[HEADS]**: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, n_heads, head_size)`\n",
        "4. **[TRANSPOSE]**: `(batch_size, sequence_length, n_heads, head_size)` $\\to$ `(batch_size, n_heads, sequence_length, head_size)`\n",
        "5. **$Q \\cdot K^T$**: `(batch_size, n_heads, sequence_length, head_size)` $\\times$ `(batch_size, n_heads, head_size, sequence_length)` $\\to$ `(batch_size, n_heads, sequence_length, sequence_length)`\n",
        "6. **[SELF-ATTENTION]**: `(batch_size, n_heads, sequence_length, sequence_length)` $\\times$ `(batch_size, n_heads, sequence_length, head_size)` $\\to$ `(batch_size, n_heads, sequence_length, head_size)`\n",
        "7. **[TRANSPOSE]**: `(batch_size, n_heads, sequence_length, head_size)` $\\to$ `(batch_size, sequence_length, n_heads, head_size)`\n",
        "8. **[MERGE]**: `(batch_size, sequence_length, n_heads, head_size)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "9. `c_proj`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "**[OUTPUT]: `(batch_size, sequence_length, n_embd)`**"
      ],
      "metadata": {
        "id": "1pmdg8X62GIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Реализация класса\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # n_embd = n_heads * head_size\n",
        "        assert config.n_embd % config.n_heads == 0, \"Please, check the divisibility of n_embd by n_heads\"\n",
        "\n",
        "        # c_attn = [w_k | w_q | w_v] (concatenated)\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(p=0.1)\n",
        "        self.resid_dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # n_heads, head_size\n",
        "        self.NH = config.n_heads\n",
        "        self.HS = config.n_embd // config.n_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, sequence_length, n_embd\n",
        "        BS, SL, EMB = x.shape\n",
        "\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(EMB, dim=2)\n",
        "\n",
        "        q = q.view(BS, SL, self.NH, self.HS).transpose(1, 2)\n",
        "        k = k.view(BS, SL, self.NH, self.HS).transpose(1, 2)\n",
        "        v = v.view(BS, SL, self.NH, self.HS).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = self.attn_dropout(y)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(BS, SL, EMB)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        y = self.resid_dropout(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "1b-3JVajuT2l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример\n",
        "\n",
        "class ExampleConfig:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_embd = 8\n",
        "        self.n_heads = 2\n",
        "\n",
        "\n",
        "config = ExampleConfig()\n",
        "m = MultiHeadAttention(config)\n",
        "\n",
        "x = torch.randn((3, 4, 8))\n",
        "print(m(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G2D69Id0qF1",
        "outputId": "8fec4f1e-961c-4226-d9e2-debbf0ab7722"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Block"
      ],
      "metadata": {
        "id": "Wo4flVuqco87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Размерности:**\n",
        "\n",
        "**[INPUT]: `(batch_size, sequence_length, n_embd)`**\n",
        "\n",
        "1. `attn`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "2. `mlp`: `(batch_size, sequence_length, n_embd)` $\\to$ `(batch_size, sequence_length, n_embd)`\n",
        "\n",
        "**[OUTPUT]: `(batch_size, sequence_length, n_embd)`**"
      ],
      "metadata": {
        "id": "9_LfUkM0h8rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Реализация класса\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # residual connection\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "s2aKDNiocqg6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример\n",
        "\n",
        "class ExampleConfig:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_embd = 8\n",
        "        self.n_heads = 2\n",
        "\n",
        "\n",
        "config = ExampleConfig()\n",
        "bl = Block(config)\n",
        "\n",
        "x = torch.randn((3, 4, 8))\n",
        "print(bl(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lidQyA6ciSoz",
        "outputId": "88df957d-6c9f-4b37-df90-9dcb8ae73e49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. GPTConfig"
      ],
      "metadata": {
        "id": "mYrJg9lwp8pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Реализация класса\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layers: int = 12 # number of layers\n",
        "    n_heads: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "UEVvxUzJqCWy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. MyGPT"
      ],
      "metadata": {
        "id": "9P9AYJrRiflb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Реализация класса\n",
        "\n",
        "class MyGPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd)\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        BS, SL = idx.shape\n",
        "\n",
        "        assert SL <= config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        pos = torch.arange(0, SL, dtype=torch.long)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layers=12, n_heads=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layers=24, n_heads=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layers=36, n_heads=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layers=48, n_heads=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = MyGPT(config)\n",
        "\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "HvFB8m__iqKW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyGPT.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8phekilqtuY",
        "outputId": "c9eaee81-eebd-4d84-ae03-99286d2c0abf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "P5NmgsRxsQVL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "num_return_sequences = 4\n",
        "max_length = 32\n",
        "\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "xgen = tokens\n",
        "sample_rng = torch.Generator()\n",
        "sample_rng.manual_seed(42)\n",
        "\n",
        "while xgen.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type='cpu', dtype=torch.bfloat16):\n",
        "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = xgen[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"rank {ddp_rank} sample {i}: {decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Jqi6baf-wN-h",
        "outputId": "664fc949-94e3-4531-f53a-c016b296db4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ExampleConfig' object has no attribute 'block_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-c2e9e6a94135>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# take the logits at the last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (B, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-5795b75346c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mSL\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ExampleConfig' object has no attribute 'block_size'"
          ]
        }
      ]
    }
  ]
}